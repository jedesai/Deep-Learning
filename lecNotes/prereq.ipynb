{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP in Deep Learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Text Data => Vectors (Numerical representation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of converting Text to Vectors\n",
    "- one hot encoding (OHE)\n",
    "- bag of words (BOG)\n",
    "- TF-IDF\n",
    "- WORD 2 VEC, AVGWORD 2 VEC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN(Artificial Neural Network):\n",
    "- This can be used for classification and regression problems.\n",
    "- Tabular Data is used here.\n",
    "- sequence of data does not matter.\n",
    "- Entire row will be sent to Neural Network at same time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN(Convolutional Neural Network):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data will be Images or Video Frames.\n",
    "- we can perform Image Classification, Object Detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN(Recurrent Neural Network):\n",
    "- Sequential Data is used.\n",
    "- Here sequence of the data is very much important.\n",
    "- We give one word at a time.\n",
    "- Text Generation, Language Translation, Chatbot Conversation, Auto Suggestion(gmail).\n",
    "- Time Series Data which is used in sales forecasting.\n",
    "- Feedback Loop is provided to all nodes, which is maintaing state/context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Function:\n",
    "- Sigmoid: used for binaray classification.\n",
    "- Softmax: used for MultiClass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "- after forward propogation, we get y^ as output. we calculate y-y^, which is our Loss Funtion. The main Aim is to reduce loss function by adjusting weights, bias (Trainable parameter) in Backward propogation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitation of Simple RNN:\n",
    "- Long term dependency can not be captured by Simple RNN.\n",
    "- output will be dependent on nearest time stemp, and avoid far time stemp.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "- To avoid above problem, we can use Relu OR leaky Relu instead of Sigmoid Activation Function.\n",
    "- Other Variant of RNN, that is LSTM RNN and GRU RNN is good to use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
